# Reinforcement Learning from Human Feedback(RLHF) Experiments
This repo contains experiment(codes, models, and datasets).

Some codes are inherited from CarperAI/trlx, lvwerra/trl, Dahoas/reward-modeling and other repos, with some modifications.

# Ref
* https://huggingface.co/blog/rlhf
* https://github.com/CarperAI/trlx
* https://huggingface.co/docs/trl/main/en/index
* https://github.com/lvwerra/trl
* https://github.com/Dahoas/reward-modeling
